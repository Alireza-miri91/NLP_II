{
  "config": {
    "split": "train",
    "k": 5,
    "limit": 200,
    "model": "sentence-transformers/all-MiniLM-L6-v2",
    "modes": [
      "embeddings",
      "bm25",
      "tfidf",
      "lsi",
      "dpr"
    ],
    "granularities": [
      "paragraph",
      "sentence"
    ],
    "include_answerability": true
  },
  "paragraph_metrics": {
    "embeddings": {
      "paragraph": {
        "metrics": {
          "precision@k": 0.020359281437125752,
          "recall@k": 0.0718562874251497,
          "f1@k": 0.02997900303289525,
          "evaluated_questions": 167
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "precision@k": 0.02634730538922156,
          "recall@k": 0.09181636726546906,
          "f1@k": 0.038144490240298624,
          "evaluated_questions": 167
        },
        "n_questions": 200
      }
    },
    "bm25": {
      "paragraph": {
        "metrics": {
          "precision@k": 0.02754491017964072,
          "recall@k": 0.0907185628742515,
          "f1@k": 0.03940776455746516,
          "evaluated_questions": 167
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "precision@k": 0.017964071856287425,
          "recall@k": 0.04810379241516966,
          "f1@k": 0.023659175156181145,
          "evaluated_questions": 167
        },
        "n_questions": 200
      }
    },
    "tfidf": {
      "paragraph": {
        "metrics": {
          "precision@k": 0.015568862275449102,
          "recall@k": 0.04161676646706587,
          "f1@k": 0.020071113184885643,
          "evaluated_questions": 167
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "precision@k": 0.025149700598802397,
          "recall@k": 0.07904191616766466,
          "f1@k": 0.03472794670399461,
          "evaluated_questions": 167
        },
        "n_questions": 200
      }
    },
    "lsi": {
      "paragraph": {
        "metrics": {
          "precision@k": 0.0011976047904191617,
          "recall@k": 0.001996007984031936,
          "f1@k": 0.0014970059880239522,
          "evaluated_questions": 167
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "precision@k": 0.0,
          "recall@k": 0.0,
          "f1@k": 0.0,
          "evaluated_questions": 167
        },
        "n_questions": 200
      }
    },
    "dpr": {
      "paragraph": {
        "metrics": {
          "precision@k": 0.020359281437125752,
          "recall@k": 0.0718562874251497,
          "f1@k": 0.02997900303289525,
          "evaluated_questions": 167
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "precision@k": 0.020359281437125752,
          "recall@k": 0.0718562874251497,
          "f1@k": 0.02997900303289525,
          "evaluated_questions": 167
        },
        "n_questions": 200
      }
    }
  },
  "granularity_compare": {
    "embeddings": {
      "mode": "embeddings",
      "n": 200,
      "paragraph_hit": 0.25,
      "sentence_hit": 0.315
    },
    "bm25": {
      "mode": "bm25",
      "n": 200,
      "paragraph_hit": 0.38,
      "sentence_hit": 0.365
    },
    "tfidf": {
      "mode": "tfidf",
      "n": 200,
      "paragraph_hit": 0.33,
      "sentence_hit": 0.34
    },
    "lsi": {
      "mode": "lsi",
      "n": 200,
      "paragraph_hit": 0.105,
      "sentence_hit": 0.09
    },
    "dpr": {
      "mode": "dpr",
      "n": 200,
      "paragraph_hit": 0.25,
      "sentence_hit": 0.25
    }
  },
  "lex_vs_sem": {
    "lexical_win": 38,
    "semantic_win": 5,
    "both_win": 53,
    "both_lose": 104,
    "total": 200
  },
  "answerability": {
    "embeddings": {
      "paragraph": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.29,
            "macro_f1": 0.2842020364956145,
            "weighted_f1": 0.33316261719931445,
            "average_confidence": 0.3046011189054843
          },
          "answerable": {
            "count": 176,
            "precision": 0.9047619047619048,
            "recall": 0.2159090909090909,
            "f1": 0.3486238532110092,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12658227848101267,
            "recall": 0.8333333333333334,
            "f1": 0.21978021978021978,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 38,
            "false_positive": 138,
            "false_negative": 4,
            "true_positive": 20
          }
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.245,
            "macro_f1": 0.24270919531583038,
            "weighted_f1": 0.27436395095162863,
            "average_confidence": 0.31298587950648654
          },
          "answerable": {
            "count": 176,
            "precision": 0.8571428571428571,
            "recall": 0.17045454545454544,
            "f1": 0.2843601895734597,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.11515151515151516,
            "recall": 0.7916666666666666,
            "f1": 0.20105820105820105,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 30,
            "false_positive": 146,
            "false_negative": 5,
            "true_positive": 19
          }
        },
        "n_questions": 200
      }
    },
    "bm25": {
      "paragraph": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.38,
            "macro_f1": 0.34537007707739414,
            "weighted_f1": 0.45979938760426564,
            "average_confidence": 0.35107543417345033
          },
          "answerable": {
            "count": 176,
            "precision": 0.8714285714285714,
            "recall": 0.3465909090909091,
            "f1": 0.4959349593495935,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.11538461538461539,
            "recall": 0.625,
            "f1": 0.19480519480519481,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 61,
            "false_positive": 115,
            "false_negative": 9,
            "true_positive": 15
          }
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.35,
            "macro_f1": 0.3256561884012864,
            "weighted_f1": 0.42303143479614064,
            "average_confidence": 0.3301272911720298
          },
          "answerable": {
            "count": 176,
            "precision": 0.8709677419354839,
            "recall": 0.3068181818181818,
            "f1": 0.453781512605042,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.11594202898550725,
            "recall": 0.6666666666666666,
            "f1": 0.19753086419753085,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 54,
            "false_positive": 122,
            "false_negative": 8,
            "true_positive": 16
          }
        },
        "n_questions": 200
      }
    },
    "tfidf": {
      "paragraph": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.12,
            "macro_f1": 0.10714285714285714,
            "weighted_f1": 0.02571428571428571,
            "average_confidence": 0.0
          },
          "answerable": {
            "count": 176,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12,
            "recall": 1.0,
            "f1": 0.21428571428571427,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 0,
            "false_positive": 176,
            "false_negative": 0,
            "true_positive": 24
          }
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.12,
            "macro_f1": 0.10714285714285714,
            "weighted_f1": 0.02571428571428571,
            "average_confidence": 0.0
          },
          "answerable": {
            "count": 176,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12,
            "recall": 1.0,
            "f1": 0.21428571428571427,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 0,
            "false_positive": 176,
            "false_negative": 0,
            "true_positive": 24
          }
        },
        "n_questions": 200
      }
    },
    "lsi": {
      "paragraph": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.12,
            "macro_f1": 0.10714285714285714,
            "weighted_f1": 0.02571428571428571,
            "average_confidence": 0.0
          },
          "answerable": {
            "count": 176,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12,
            "recall": 1.0,
            "f1": 0.21428571428571427,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 0,
            "false_positive": 176,
            "false_negative": 0,
            "true_positive": 24
          }
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.12,
            "macro_f1": 0.10714285714285714,
            "weighted_f1": 0.02571428571428571,
            "average_confidence": 0.0
          },
          "answerable": {
            "count": 176,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12,
            "recall": 1.0,
            "f1": 0.21428571428571427,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 0,
            "false_positive": 176,
            "false_negative": 0,
            "true_positive": 24
          }
        },
        "n_questions": 200
      }
    },
    "dpr": {
      "paragraph": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.29,
            "macro_f1": 0.2842020364956145,
            "weighted_f1": 0.33316261719931445,
            "average_confidence": 0.3046011189054843
          },
          "answerable": {
            "count": 176,
            "precision": 0.9047619047619048,
            "recall": 0.2159090909090909,
            "f1": 0.3486238532110092,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12658227848101267,
            "recall": 0.8333333333333334,
            "f1": 0.21978021978021978,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 38,
            "false_positive": 138,
            "false_negative": 4,
            "true_positive": 20
          }
        },
        "n_questions": 200
      },
      "sentence": {
        "metrics": {
          "overall": {
            "total_questions": 200,
            "accuracy": 0.29,
            "macro_f1": 0.2842020364956145,
            "weighted_f1": 0.33316261719931445,
            "average_confidence": 0.3046011189054843
          },
          "answerable": {
            "count": 176,
            "precision": 0.9047619047619048,
            "recall": 0.2159090909090909,
            "f1": 0.3486238532110092,
            "support": 176
          },
          "unanswerable": {
            "count": 24,
            "precision": 0.12658227848101267,
            "recall": 0.8333333333333334,
            "f1": 0.21978021978021978,
            "support": 24
          },
          "confusion_matrix": {
            "true_negative": 38,
            "false_positive": 138,
            "false_negative": 4,
            "true_positive": 20
          }
        },
        "n_questions": 200
      }
    }
  },
  "started_at": 1759216893.8407772,
  "finished_at": 1759223824.3970416,
  "duration_sec": 6930.556264400482
}